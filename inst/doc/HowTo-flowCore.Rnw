% NOTE -- ONLY EDIT THE .Rnw FILE!!!  The .tex file is
% likely to be overwritten.
%
%\VignetteIndexEntry{Basic Functions for Flow Cytometry Data}
%\VignetteDepends{flowCore}
%\VignetteKeywords{}
%\VignettePackage{flowCore}
\documentclass[11pt]{article}

\usepackage{times}
\usepackage{hyperref}
\usepackage[authoryear,round]{natbib}
\usepackage{times}
\usepackage{comment}
\usepackage{graphicx}
\usepackage{subfigure}

\textwidth=6.2in
\textheight=8.5in
\oddsidemargin=.1in
\evensidemargin=.1in
\headheight=-.3in

\newcommand{\scscst}{\scriptscriptstyle}
\newcommand{\scst}{\scriptstyle}
\newcommand{\Rfunction}[1]{{\texttt{#1}}}
\newcommand{\Robject}[1]{{\texttt{#1}}}
\newcommand{\Rpackage}[1]{{\textsf{#1}}}
\newcommand{\Rclass}[1]{{\textit{#1}}}



\title{flowCore: data structures pacakge for flow cytometry data}
\author{N. Le Meur F. Hahne B. Ellis P. Haaland}

\begin{document}
\maketitle

\begin{abstract}
\noindent \textbf{Background} The recent application of modern automation technologies to staining and collecting flow cytometry (FCM) samples has led to many new challenges in data management and analysis. We limit our attention here to the associated problems in the analysis of the massive amounts of FCM data now being collected. From our viewpoint, see two related but substantially different problems arising. On the one hand, there is the problem of adapting existing software to apply standard methods to the increased volume of data. The second problem, which we intend to address here, is the absence of any research platform which bioinformaticians, computer scientists, and statisticians can use to develop novel methods that address both the volume and multidimensionality of the mounting tide of data. In our opinion, such a platform should be Open Source, be focused on visualization,  support rapid prototyping, have a large existing base of users, and have demonstrated suitability for development of new methods. We believe that the Open Source statistical software R in conjunction with the Bioconductor Project fills all of these requirements. Consequently we have developed a Bioconductor package that we call flowCore. The flowCore package is not intended to be a complete analysis package for FCM data, rather, we see it as providing a clear object model and a collection of standard tools that enable R as an informatics research platform for flow cytometry. One of the important issues that we have addressed in the flowCore package is that of using a standardized representation that will insure compatibility with existing technologies for data analysis and will support collaboration and interoperability of new methods as they are developed. In order to do this, we have followed the current standardized descriptions of FCM data analysis as being developed under NIH Grant xxxx [n]. We believe that researchers will find flowCore to be a solid foundation for future development of new methods to attack the many interesting open research questions in FCM data analysis.\\
\textbf{Methods} We propose a variety different data structures. We have implemented the classes and methods in the Bioconductor package flowCore. 
We illustrate their use with X case studies.\\
\textbf{Results} We hope that those proposed data structures will the base for the development of many tools for the analysis of high throughput flowcytometry.\\
\textbf{keywords} Flow cytometry, high throughtput, software, standard
\end{abstract}


\section{Introduction}
Traditionally, flow cytometry has been a tube-based technique limited to small-scale laboratory and clinical studies. 
High throughput methods for flow cytometry have recently been developed for drug discovery and advanced research methods 
\cite{Gasparetto2004, Young2004}. As an example, the flow cytometry high content screening (FC-HCS) can process up 
to a thousand samples daily at a single workstation, and the results have been equivalent or superior to traditional 
manual multiparameter staining and analysis techniques. \\

The amount of information generated by high throughput technologies such as FC-HCS need to be transformed into 
executive summaries (which are brief enough) for creative studies by a human researcher \cite{Brazma2001}. 
Standardization is critical when developing new high throughput technologies and their associated information services 
\cite{Brazma2001, Chicurel2002, Boguski2003}. Standardization efforts have been made in clinical cell analysis 
by flow cytometry \cite{Keeney2004}, however data interpretation has not been standardized for even low throughput FCM. 
It is one of the most difficult and time consuming aspects of the entire analytical process as well as a primary source 
of variation in clinical tests, and investigators have traditionally relied on intuition rather than standardized statistical inference 
\cite{Bagwell2004,Braylan2004,Parks1997,Suni2003}. In the development of standards in high throughput FCM, 
few progress has been done in term of Open Source software. In this article we propose R data structures to handle 
flow cytometry data through the main steps of preprocessing: compensation, transformation, filtering.\\

The aim is to merge both prada  and rflowcyt \cite{LeMeur2006} into one core package wich is compliant 
with the data exchange standards that are currently developed in the community \cite{Spidlen2006}.

Visualization as well as quality control will than be part of the utility packages that depend on the data structures defined in the flowCore package.


\section{Data model}
The package \Rpackage{flowCore} offers the possibility to store single measurement (FCS file) or combine several individual
measurements in the confines of a single experiment and to include all the necessary metadata. Its object
model tries to stay close to the familiar micro-array data structures (expressionSet) making use of already
defined generic functions.\\

\subsection{flowFrame}
The objects of class \Robject{flowFrame} are the containers for storing individual 
cytometry measurements originated from one FCS file. The information available are:\\
%%need description of the slots and maybe methods associated
\begin{itemize}
\item exprs, containing the measured intensities. Rows correspond to cells, columns to the
      different channels. 
\item parameters, containing information about each column of the  flowFrame. This will generally
      be filled in by \Rfunction{read.FCS} or similar functions using data from
      the \Robject{description} slot that describes the parameters.
\item description is list containing the meta data included in the FCS file.
\end{itemize}

\subsection{flowSet}
An flow cytometry experiment usually include several sample and several measurements. 
A data structure is thus needed to store those collections of measurements.  
The package \Rpackage{flowCore} stores the measurement and the associated data 
information in objects of class \Robject{flowSet}, a collection of \Robject{flowFrame}. 
A \Robject{flowSet} object contains two main slots \Robject{frames}, an environment data structure 
that stores the flow cytometry measurements as \Robject{flowFrame} and the \Robject{phenoData}. 
The \Robject{phenoData} slot contains all the relevant experiment-wide meta-data, \textit{e.g.} 
phenotypic and experimental information relevant to the management and/or the analysis of the data.\\

\section{Preprocessing Methods}
\subsection{Compensation}
Fluorescence compensation is the process by which the amounts of spectral overlap are 
estimated and subtracted from the total detected signals to yield an estimate of the 
actual amount of each dye \cite{Herzenberg2006}. Single-laser, two-color FACS experiments 
in the mid-1970s showed that fluorescence compensation is important. 
It is now recognized as an obligate step in most FACS analyses.
%%definition: Compensation vs spillover

\subsection{Transformation}
While analyzing flow cytometry data, various parameter transformations are performed 
to provide user-friendly visualization and/or to perform statistical analyses and interpret the data. 
The proposed standard \cite{Spidlen2006} defined 8 transformations communly used: linear, quadratic, natural logarithm, 
logarithm (with a specified base), hyperlog, bi-exponential, logicle, split-scale. All of them are 
implemented in \Rpackage{flowcore} and a few other have been added. Each transformation has specific properties:\\ 

%%could be a table
\begin{itemize}
\item linear
\item logarithm
\item biexponential
\item logicle transformation help to visualize cells with minimal fluorescence 
  (cells usually pilled up on the axis with logarithmic displays) \citep{Herzenberg2006}
\item arcsinh 
\end{itemize}


\subsection{Filter}
The role of filtering strategies are to identify and characterize different cells population in sample.
%% Filtering vs Gating
%%Byron's vignette?
%%several approach from basic to DAG ...\\

\section{Case study}
\subsection{dataset}
Our experiment is composed of 5 96-well plates\\
%%Perry can we use that example or do you prefere an other one? 
%% Can you add a paragraph describing the experiment?

First we load the library and read in some experimental data files.
<<loadlib, echo=TRUE, results= hide>>=
library(flowCore)
@ 

<<readData, echo=TRUE, results=hide>>=
fcs.loc <- system.file("extdata/compdata/data", package="flowCore")
file.location <- paste(fcs.loc, dir(fcs.loc), sep="/")
experiment <- read.flowSet(file.location, transformation = "linearize")
experiment
@ 
\subsection{Quality Assessment}
Quickly look at the range of the intensities between samples 
other statistics sur as ... cf Janet paper.
<<summaryRange>>=
rang <- round(fsApply(experiment, each_col, range), 2)
rownames(rang) <- paste(rep(dir(fcs.loc), each=2), c("lower","upper") ,sep="")
rang
@ 

\subsection{Preprocessing}
<<compensationMatrix, echo=FALSE, results= hide>>=
comp.loc <- system.file("extdata/compdata", package="flowCore")
comp.location <- paste(comp.loc, dir(comp.loc), sep="/")
comp.mat <- as.matrix(read.table(comp.location[1], header=TRUE, skip=2, check.names = FALSE))
@ 

Compensation is ....
<<compensateExp, echo=TRUE>>=
expCompensate <- compensate(experiment,comp.mat)
@ 

Transformation
<<scaleScatter, echo=TRUE>>=
scaleTrans <- scaleTransform("scatter-scale", a=0,  b=1023)
expCompensate <- transform(expCompensate,`FSC-H`= scaleTrans(`FSC-H`), `SSC-H`= scaleTrans(`SSC-H`))
@ 

Filter to leave out debris:
<<rectangleFilter, echo=TRUE>>=
filterDebris <- rectangleGate("FSC-H"=c(0.2, 1), "SSC-H"=c(0.04, 1))
expResult <- fsApply(expCompensate, function(x) filter(x, filterDebris))

cell <- Subset(expCompensate[[1]],expResult[[1]])
@ 

<<plotWithDebris, echo=FALSE, fig=TRUE, prefix=FALSE, include=FALSE>>=
print(plot(expCompensate[[1]],  xlim=c(0, 1), ylim=c(0, 1), 
main="File1 (compensated, scaled)"))
@ 

<<plotCell,echo=False, fig=TRUE, prefix=FALSE, include=FALSE>>=
print(plot(cell, xlim=c(0, 1), ylim=c(0, 1), 
     main="File1 (compensated, scaled, debris removed)"))
@ 

\begin{figure}
\centering
\subfigure{\includegraphics[width=40mm]{plotWithDebris} A}
\subfigure{\includegraphics[width=40mm]{plotCell} B}
\caption{
  \label{fig:Fig01}{Example of smooth scatter plot of FSC-H
    \textit{vs.} SSC-H data (A) before and (B) after a applying a rectangle 
on the  measurements (compensated and scaled) extracted from File1}} 
\end{figure}


\section{Discussion and Conclusion}
%% Explain the usefullness of a commun data structure maybe in comparaison with the ExpressionSet developed for
%% microarray - count how manay package in BioC depend on the  ExpressionSet data structure?
%%flowViz
%%flowQ etc

\section*{Acknowledgements}
We would like to thank R. Gentleman RR. Brinkman
%%or should they be Authors

\clearpage
\bibliographystyle{plainnat} 
\bibliography{cytoref}
\clearpage
\small{
<<sessionInfo>>=
sessionInfo()
@ 
}
\end{document}
